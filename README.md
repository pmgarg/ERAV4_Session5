
```bash


### View All Experiments
| Experiment | Notebook | Best Accuracy | View on |
|------------|----------|---------------|----------|
| **Exp 1: SGD + Scheduler** | [`Session5_model_training_SGD.ipynb`](Session5_model_training_SGD.ipynb) | 99.56% |
| **Exp 2: SGD Fixed LR** | [`Session5_model_training_SGD_no_scheduler.ipynb`](Session5_model_training_SGD_no_scheduler.ipynb) | 99.10%  |
| **Exp 3: Adam + Scheduler** â­ | [`Session5_model_training_adam.ipynb`](Session5_model_training_adam.ipynb) | **99.61%** |
| **Exp 4: AdamW + Scheduler** | [`Session5_model_training_adamw.ipynb`](Session5_model_training_adamw.ipynb) | 99.60% |
## ğŸ“Š Model Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: 28Ã—28Ã—1 MNIST Image                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conv Block 1: Feature Extraction                   â”‚
â”‚  â”œâ”€ Conv1 (1â†’14) + BN â†’ 26Ã—26Ã—14                  â”‚
â”‚  â”œâ”€ Conv2 (14â†’14) + BN â†’ 24Ã—24Ã—14                 â”‚
â”‚  â””â”€ Conv3 (14â†’16) + BN â†’ 22Ã—22Ã—16                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MaxPool2d(2Ã—2) â†’ 11Ã—11Ã—16                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conv Block 2: Deep Features                        â”‚
â”‚  â”œâ”€ Conv4 (16â†’18) + BN â†’ 9Ã—9Ã—18                   â”‚
â”‚  â””â”€ Conv5 (18â†’20) + BN â†’ 7Ã—7Ã—20                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MaxPool2d(2Ã—2) â†’ 3Ã—3Ã—20                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conv Block 3: Final Features                       â”‚
â”‚  â”œâ”€ Conv6 (20â†’22) + BN â†’ 3Ã—3Ã—22                   â”‚
â”‚  â””â”€ Conv7 (22â†’22) + BN â†’ 1Ã—1Ã—22                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Global Average Pooling â†’ 22                       â”‚
â”‚  Fully Connected â†’ 10 classes                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Training Evolution: The Quest for 99.4%

### ğŸ“ˆ Experiment Timeline

<table>
<tr>
<td>

#### ğŸ”¬ Experiment 1: Baseline SGD
ğŸ““ **Notebook:** [`Session5_model_training_SGD.ipynb`](./Session5_model_training_SGD.ipynb)

**Configuration:**
- Optimizer: `SGD`
- Learning Rate: `0.006`
- Weight Decay: `1e-6`
- Scheduler: `OneCycleLR`
- Max LR: `0.02`

</td>
<td>

```python
# Performance Metrics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Epoch   â”‚  Train   â”‚   Test   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1     â”‚  85.66%  â”‚  98.20%  â”‚
â”‚    5     â”‚  98.49%  â”‚  99.21%  â”‚
â”‚   10     â”‚  98.96%  â”‚  99.47%  â”‚
â”‚   15     â”‚  99.18%  â”‚  99.50%  â”‚
â”‚   20     â”‚  99.39%  â”‚  99.56%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

#### ğŸ† Achievement Unlocked: **99.56%** Test Accuracy! âœ¨

---

<table>
<tr>
<td>

#### ğŸ”¬ Experiment 2: SGD Without Scheduler
ğŸ““ **Notebook:** [`Session5_model_training_SGD_no_scheduler.ipynb`](./Session5_model_training_SGD_no_scheduler.ipynb)

**Configuration:**
- Optimizer: `SGD`
- Learning Rate: `0.06` (fixed)
- Weight Decay: `1e-4`
- Scheduler: `None`

</td>
<td>

```python
# Performance Metrics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Epoch   â”‚  Train   â”‚   Test   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1     â”‚  64.31%  â”‚  90.57%  â”‚
â”‚    5     â”‚  96.08%  â”‚  98.26%  â”‚
â”‚   10     â”‚  97.68%  â”‚  98.63%  â”‚
â”‚   15     â”‚  98.15%  â”‚  99.02%  â”‚
â”‚   20     â”‚  98.47%  â”‚  99.10%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

#### ğŸ“Š Result: **99.10%** - Slower convergence without scheduler

---

<table>
<tr>
<td>

#### ğŸ”¬ Experiment 3: Adam with Scheduler
ğŸ““ **Notebook:** [`Session5_model_training_adam.ipynb`](./Session5_model_training_adam.ipynb)

**Configuration:**
- Optimizer: `Adam`
- Learning Rate: `0.01`
- Weight Decay: `1e-4`
- Scheduler: `OneCycleLR`

</td>
<td>

```python
# Performance Metrics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Epoch   â”‚  Train   â”‚   Test   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1     â”‚  92.00%  â”‚  98.17%  â”‚
â”‚    5     â”‚  98.00%  â”‚  98.97%  â”‚
â”‚   10     â”‚  98.12%  â”‚  99.01%  â”‚
â”‚   15     â”‚  98.75%  â”‚  99.42%  â”‚
â”‚   20     â”‚  99.41%  â”‚  99.61%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

#### ğŸ‰ New Record: **99.61%** Test Accuracy! ğŸ¥‡

---

<table>
<tr>
<td>

#### ğŸ”¬ Experiment 4: AdamW with Scheduler
ğŸ““ **Notebook:** [`Session5_model_training_adamw.ipynb`](./Session5_model_training_adamw.ipynb)

**Configuration:**
- Optimizer: `AdamW`
- Learning Rate: `0.02`
- Weight Decay: `1e-4`
- Scheduler: `OneCycleLR`

</td>
<td>

```python
# Performance Metrics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Epoch   â”‚  Train   â”‚   Test   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1     â”‚  92.28%  â”‚  96.43%  â”‚
â”‚    5     â”‚  98.41%  â”‚  98.80%  â”‚
â”‚   10     â”‚  98.86%  â”‚  99.39%  â”‚
â”‚   15     â”‚  99.22%  â”‚  99.55%  â”‚
â”‚   20     â”‚  99.53%  â”‚  99.60%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

#### ğŸ¯ Excellent Performance: **99.60%** Test Accuracy! 

---

## ğŸ“Š Comparative Analysis

### Convergence Speed Comparison

```
Test Accuracy Progress:
      99.6% â”¤                                    â•­â”€â”€â”€â”€â”€
      99.4% â”¤                         â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
      99.2% â”¤                   â•­â”€â”€â”€â”€â”€â•¯
      99.0% â”¤            â•­â”€â”€â”€â”€â”€â”€â•¯
      98.8% â”¤       â•­â”€â”€â”€â”€â•¯
      98.6% â”¤   â•­â”€â”€â”€â•¯
      98.4% â”¤  â•±
      98.2% â”¤ â•±
            â””â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€
              1    3    5    7    9   11   13   15   20
                            Epochs

    â”€â”€â”€ Exp 1 (SGD+Scheduler)    Best: 99.56%
    â”€â”€â”€ Exp 2 (SGD Only)         Best: 99.10%
    â”€â”€â”€ Exp 3 (Adam+Scheduler)   Best: 99.61% â­
    â”€â”€â”€ Exp 4 (AdamW+Scheduler)  Best: 99.60%
```

## ğŸ”‘ Key Insights

### âœ… What Worked Well

1. **OneCycleLR Scheduler** 
   - Dramatically improved convergence speed
   - Better final accuracy across all optimizers

2. **Adam Optimizer**
   - Fastest initial convergence
   - Achieved best overall accuracy (99.61%)
   
3. **Batch Normalization**
   - Stabilized training across all experiments
   - No signs of overfitting even without dropout

### ğŸ“ˆ Performance Summary

| Metric | Exp 1 | Exp 2 | Exp 3 | Exp 4 |
|--------|-------|-------|-------|-------|
| **Best Test Acc** | 99.56% | 99.10% | **99.61%** ğŸ† | 99.60% |
| **Best Train Acc** | 99.39% | 98.47% | 99.41% | **99.53%** |
| **Epochs to 99%** | 4 | 15 | 10 | 9 |
| **Training Stability** | High | Medium | High | High |

## ğŸš€ Model Efficiency Stats

<div align="center">

| **Metric** | **Value** | **Status** |
|------------|-----------|------------|
| Total Parameters | 18,662 | âœ… Under 25K limit |
| Model Size | ~73 KB | ğŸ¯ Ultra-lightweight |
| Training Time/Epoch | ~6.5 seconds | âš¡ Fast |
| Inference Time | <1 ms | ğŸš€ Real-time capable |
| Memory Usage | <50 MB | ğŸ’š Mobile-ready |

</div>

## ğŸ—ï¸ Detailed Architecture Breakdown

### Layer-by-Layer Analysis

| Layer | Type | Parameters | Input Size | Output Size | Receptive Field |
|-------|------|-----------|------------|-------------|-----------------|
| **conv1** | Conv2d(1â†’14, k=3) | (3Ã—3Ã—1+1)Ã—14 = 140 | 28Ã—28Ã—1 | 26Ã—26Ã—14 | 3Ã—3 |
| **bn1** | BatchNorm2d | 14Ã—2 = 28 | 26Ã—26Ã—14 | 26Ã—26Ã—14 | 3Ã—3 |
| **conv2** | Conv2d(14â†’14, k=3) | (3Ã—3Ã—14+1)Ã—14 = 1,778 | 26Ã—26Ã—14 | 24Ã—24Ã—14 | 5Ã—5 |
| **bn2** | BatchNorm2d | 14Ã—2 = 28 | 24Ã—24Ã—14 | 24Ã—24Ã—14 | 5Ã—5 |
| **conv3** | Conv2d(14â†’16, k=3) | (3Ã—3Ã—14+1)Ã—16 = 2,032 | 24Ã—24Ã—14 | 22Ã—22Ã—16 | 7Ã—7 |
| **bn3** | BatchNorm2d | 16Ã—2 = 32 | 22Ã—22Ã—16 | 22Ã—22Ã—16 | 7Ã—7 |
| **pool1** | MaxPool2d(2Ã—2) | 0 | 22Ã—22Ã—16 | 11Ã—11Ã—16 | 14Ã—14 |
| **conv4** | Conv2d(16â†’18, k=3) | (3Ã—3Ã—16+1)Ã—18 = 2,610 | 11Ã—11Ã—16 | 9Ã—9Ã—18 | 18Ã—18 |
| **bn4** | BatchNorm2d | 18Ã—2 = 36 | 9Ã—9Ã—18 | 9Ã—9Ã—18 | 18Ã—18 |
| **conv5** | Conv2d(18â†’20, k=3) | (3Ã—3Ã—18+1)Ã—20 = 3,260 | 9Ã—9Ã—18 | 7Ã—7Ã—20 | 22Ã—22 |
| **bn5** | BatchNorm2d | 20Ã—2 = 40 | 7Ã—7Ã—20 | 7Ã—7Ã—20 | 22Ã—22 |
| **pool2** | MaxPool2d(2Ã—2) | 0 | 7Ã—7Ã—20 | 3Ã—3Ã—20 | 44Ã—44 |
| **conv6** | Conv2d(20â†’22, k=3, p=1) | (3Ã—3Ã—20+1)Ã—22 = 3,982 | 3Ã—3Ã—20 | 3Ã—3Ã—22 | 44Ã—44 |
| **bn6** | BatchNorm2d | 22Ã—2 = 44 | 3Ã—3Ã—22 | 3Ã—3Ã—22 | 44Ã—44 |
| **conv7** | Conv2d(22â†’22, k=3) | (3Ã—3Ã—22+1)Ã—22 = 4,378 | 3Ã—3Ã—22 | 1Ã—1Ã—22 | 48Ã—48 |
| **bn7** | BatchNorm2d | 22Ã—2 = 44 | 1Ã—1Ã—22 | 1Ã—1Ã—22 | 48Ã—48 |
| **gap** | AdaptiveAvgPool2d | 0 | 1Ã—1Ã—22 | 1Ã—1Ã—22 | Full |
| **fc** | Linear(22â†’10) | 22Ã—10+10 = 230 | 22 | 10 | Full |

**Total Parameters: 18,662**

## ğŸ’¡ Architecture Decisions Explained

### Core Architecture Components

* **How many layers?** 7Ã—Conv (all 3Ã—3), 7Ã—BN, 2Ã—MaxPool, 1Ã—GAP, 1Ã—FC
* **MaxPooling:** 2 layers (after C3 and after C5)
* **1Ã—1 Convolutions:** None used
* **3Ã—3 Convolutions:** 7 (all convs are 3Ã—3)
* **Softmax:** Not in model; `CrossEntropyLoss` applies `log_softmax` internally. Use `softmax` only for reporting probabilities

### Training Configuration Insights

* **Learning rate:** 
  - SGD base lr 0.006 with OneCycleLR(max_lr=0.02)
  - weight_decay=1e-4 
  - Adamw lr 0.001 with OneCycleLR(max_lr=0.02)
  - weight_decay=1e-4
  

* **Kernels & Channel Progression:** 14â†’14â†’16â†’18â†’20â†’22â†’22
  - Modest growth to stay within ~19k params

### Regularization & Normalization

* **BatchNorm:** After every conv, before ReLU
  - Stabilizes training
  - Allows higher learning rates
  - Adds tiny regularization effect

* **Image normalization:** `transforms.Normalize((0.1307,), (0.3081,))` for MNIST
  - Dataset-specific mean/std

* **Dropout:** None in this model
  - BN + label smoothing (0.05) sufficient for MNIST
  - Introduce small dropout (0.05â€“0.1) near head **only if** trainâ‰«test (clear overfitting)

### Architectural Positioning

* **Position of MaxPooling:** 
  - First Pool after 3 Conv layers
  - Second pool after next 2 conv layer

* **Transition layers (1Ã—1 conv):** Not used here

* **"Distance" of MaxPool from prediction:** 
  - Last pool â†’ **2 convs** â†’ (GAPâ†’FC)

* **"Distance" of BN from prediction:**
  - BN7 is right before GAP/FC (BNâ†’ReLUâ†’C7â†’BNâ†’ReLUâ†’GAPâ†’FC)





</div>